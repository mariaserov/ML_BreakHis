{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1.0\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "print(PIL.__version__)  # Check if it’s installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting image for filepath.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"../data/train.csv\")  # Change to your actual file path\n",
    "\n",
    "# Define output folder to save images\n",
    "output_folder = \"../data/extracted_images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to load and save images\n",
    "def extract_images(df, output_folder, target_size=(150, 150)):\n",
    "    for idx, row in df.iterrows():\n",
    "        filepath = row[\"filepath\"]\n",
    "        image = cv2.imread(filepath)  # Read image\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, target_size)  # Resize\n",
    "            save_path = os.path.join(output_folder, f\"image_{idx}.png\")\n",
    "            cv2.imwrite(save_path, image)  # Save image\n",
    "            print(f\"Saved: {save_path}\")\n",
    "        else:\n",
    "            print(f\"Could not load: {filepath}\")\n",
    "\n",
    "# Extract and save images\n",
    "extract_images(df, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_image(filepath, target_size=(128, 128)):\n",
    "    \"\"\"Load and preprocess an image from the given file path.\"\"\"\n",
    "    image = cv2.imread(filepath)  # Read the image\n",
    "    if image is None:\n",
    "        return None  # Skip missing/corrupt images\n",
    "    image = cv2.resize(image, target_size)  # Resize image\n",
    "    image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def get_image_data(path, x_label=\"filepath\", y_label=\"label\"):\n",
    "    df = pd.read_csv(path)\n",
    "    print(df.columns)\n",
    "    X = []\n",
    "    y = []\n",
    "    for filepath, label in df[['filepath', 'label']].values:  # Ignore magnification for now\n",
    "        image = preprocess_image(filepath)\n",
    "        if image is not None:\n",
    "            X.append(image)\n",
    "            y.append(1 if label == \"malignant\" else 0)  # Convert labels to binary (0: benign, 1: malignant)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'filepath', 'label', 'magnification', 'tumor_subtype'], dtype='object')\n",
      "Loaded 10 images and 10 labels.\n",
      "Image shape: (10, 128, 128, 3), Label shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "X, y = get_image_data(\"../data/toy_dataset.csv\")\n",
    "print(f\"Loaded {len(X)} images and {len(y)} labels.\")\n",
    "print(f\"Image shape: {X.shape}, Label shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if file exist with given file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All image files exist!\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_path = \"../data/toy_dataset.csv\"  # Update with your actual CSV file path\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Check if file paths exist\n",
    "df[\"file_exists\"] = df[\"filepath\"].apply(os.path.isfile)\n",
    "\n",
    "# Print missing files\n",
    "missing_files = df[~df[\"file_exists\"]]\n",
    "\n",
    "if not missing_files.empty:\n",
    "    print(f\"❌ {len(missing_files)} missing image files detected!\")\n",
    "    print(missing_files[[\"filepath\"]].head())  # Show first few missing files\n",
    "else:\n",
    "    print(\"✅ All image files exist!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"../data/toy_dataset.csv\")  # CSV containing file paths & labels\n",
    "\n",
    "save_dir = \"../data/augmented_images\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "\n",
    "# Create an image data generator with augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 45,   # Rotate images up to 30 degrees\n",
    "    horizontal_flip = True,   # Flip images horizontally\n",
    "    rescale=1./255      #normalise pixel values\n",
    ")\n",
    "\n",
    "\n",
    "# Load images in batches\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    directory = \"\" ,  # Folder where images are stored\n",
    "    x_col=\"filepath\",  # Column containing image file paths\n",
    "    y_col=\"label\",  # Column with target labels (label or tumor_subtype)\n",
    "    \n",
    "    #target_size=(150, 150),  # resize image\n",
    "    batch_size=32,  # 32 images per batch\n",
    "    class_mode='binary',  #outcome ('categprical' for multiclass)\n",
    "\n",
    "    save_to_dir=save_dir,      # Save augmented images\n",
    "    save_prefix='aug',         # Prefix for saved images\n",
    "    save_format='png'         # Format of saved images\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset saved to ../data/augmented_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List to store new image metadata\n",
    "augmented_data = []\n",
    "\n",
    "# Process each image in the dataset\n",
    "for index, row in train_df.iterrows():\n",
    "    img_path = row[\"filepath\"]\n",
    "    \n",
    "    # Load image\n",
    "    try:\n",
    "        image = load_img(img_path)  # Load original image\n",
    "        image = img_to_array(image)  # Convert to array\n",
    "        image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Generate one augmented image\n",
    "        batch = next(datagen.flow(image, batch_size=1))\n",
    "        new_filename = f\"aug_{index}.png\"\n",
    "        new_filepath = os.path.join(save_dir, new_filename)\n",
    "\n",
    "        # Save the augmented image\n",
    "        array_to_img(batch[0]).save(new_filepath)\n",
    "\n",
    "        # Append new metadata row\n",
    "        augmented_data.append([new_filepath] + row.tolist()[1:])  # Keep original metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# Create new DataFrame with augmented data\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=train_df.columns)\n",
    "\n",
    "# Save new CSV with augmented image paths & metadata\n",
    "augmented_df.to_csv(\"../data/augmented_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"Augmented dataset saved to ../data/augmented_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of images in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 10\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../data/augmented_images\"\n",
    "image_count = len([f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "print(f\"Total images: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of       Unnamed: 0                                           filepath  \\\n",
       "0              0  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "1              1  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "2              2  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "3              3  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "4              4  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "...          ...                                                ...   \n",
       "7904        7904  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "7905        7905  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "7906        7906  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "7907        7907  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "7908        7908  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "\n",
       "          label magnification      tumor_subtype  \n",
       "0        benign          100X    tubular_adenoma  \n",
       "1        benign          100X    tubular_adenoma  \n",
       "2        benign          100X    tubular_adenoma  \n",
       "3        benign          100X    tubular_adenoma  \n",
       "4        benign          100X    tubular_adenoma  \n",
       "...         ...           ...                ...  \n",
       "7904  malignant          200X  lobular_carcinoma  \n",
       "7905  malignant          200X  lobular_carcinoma  \n",
       "7906  malignant          200X  lobular_carcinoma  \n",
       "7907  malignant          200X  lobular_carcinoma  \n",
       "7908  malignant          200X  lobular_carcinoma  \n",
       "\n",
       "[7909 rows x 5 columns]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "try_data = pd.read_csv(\"../data/metadata.csv\")\n",
    "try_data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>magnification</th>\n",
       "      <th>tumor_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5445</td>\n",
       "      <td>../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...</td>\n",
       "      <td>malignant</td>\n",
       "      <td>200X</td>\n",
       "      <td>ductal_carcinoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>558</td>\n",
       "      <td>../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...</td>\n",
       "      <td>benign</td>\n",
       "      <td>200X</td>\n",
       "      <td>tubular_adenoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5308</td>\n",
       "      <td>../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...</td>\n",
       "      <td>malignant</td>\n",
       "      <td>200X</td>\n",
       "      <td>ductal_carcinoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4521</td>\n",
       "      <td>../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...</td>\n",
       "      <td>malignant</td>\n",
       "      <td>400X</td>\n",
       "      <td>ductal_carcinoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7230</td>\n",
       "      <td>../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...</td>\n",
       "      <td>malignant</td>\n",
       "      <td>40X</td>\n",
       "      <td>ductal_carcinoma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           filepath      label  \\\n",
       "0        5445  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...  malignant   \n",
       "1         558  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...     benign   \n",
       "2        5308  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...  malignant   \n",
       "3        4521  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...  malignant   \n",
       "4        7230  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...  malignant   \n",
       "\n",
       "  magnification     tumor_subtype  \n",
       "0          200X  ductal_carcinoma  \n",
       "1          200X   tubular_adenoma  \n",
       "2          200X  ductal_carcinoma  \n",
       "3          400X  ductal_carcinoma  \n",
       "4           40X  ductal_carcinoma  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of       Unnamed: 0                                           filepath  \\\n",
       "0              0  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "1              1  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "2              2  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "3              3  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "4              4  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "...          ...                                                ...   \n",
       "7904        7904  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "7905        7905  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "7906        7906  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "7907        7907  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "7908        7908  ../data/versions/4/BreaKHis_v1/BreaKHis_v1/his...   \n",
       "\n",
       "          label magnification      tumor_subtype  \n",
       "0        benign          100X    tubular_adenoma  \n",
       "1        benign          100X    tubular_adenoma  \n",
       "2        benign          100X    tubular_adenoma  \n",
       "3        benign          100X    tubular_adenoma  \n",
       "4        benign          100X    tubular_adenoma  \n",
       "...         ...           ...                ...  \n",
       "7904  malignant          200X  lobular_carcinoma  \n",
       "7905  malignant          200X  lobular_carcinoma  \n",
       "7906  malignant          200X  lobular_carcinoma  \n",
       "7907  malignant          200X  lobular_carcinoma  \n",
       "7908  malignant          200X  lobular_carcinoma  \n",
       "\n",
       "[7909 rows x 5 columns]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dense, GlobalAveragePooling2D, Add\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "#defined function to build convolution block\n",
    "\n",
    "def conv_block(x, filters, kernel_size, strides, padding='same'):\n",
    "    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined identity_block and projection block\n",
    "\n",
    "def identity_block(x, filters):\n",
    "    shortcut = x\n",
    "    x = conv_block(x, filters=filters, kernel_size=(1, 1), strides=(1, 1))\n",
    "    x = conv_block(x, filters=filters, kernel_size=(3, 3), strides=(1, 1))\n",
    "    x = Conv2D(filters=filters * 4, kernel_size=(1, 1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def projection_block(x, filters, strides):\n",
    "    shortcut = x\n",
    "    x = conv_block(x, filters=filters, kernel_size=(1, 1), strides=strides)\n",
    "    x = conv_block(x, filters=filters, kernel_size=(3, 3), strides=(1, 1))\n",
    "    x = Conv2D(filters=filters * 4, kernel_size=(1, 1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    shortcut = Conv2D(filters=filters * 4, kernel_size=(1, 1), strides=strides)(shortcut)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(224, 224, 3), num_classes=1000):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # initial conv layer\n",
    "    x = conv_block(inputs, filters=64, kernel_size=(7, 7), strides=(2, 2), padding='')\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # conv block 1\n",
    "    x = projection_block(x, filters=64, strides=(1, 1))\n",
    "    x = identity_block(x, filters=64)\n",
    "    x = identity_block(x, filters=64)\n",
    "\n",
    "    # conv block 2\n",
    "    x = projection_block(x, filters=128, strides=(2, 2))\n",
    "    x = identity_block(x, filters=128)\n",
    "    x = identity_block(x, filters=128)\n",
    "    x = identity_block(x, filters=128)\n",
    "\n",
    "    # conv block 3\n",
    "    x = projection_block(x, filters=256, strides=(2, 2))\n",
    "    x = identity_block(x, filters=256)\n",
    "    x = identity_block(x, filters=256)\n",
    "    x = identity_block(x, filters=256)\n",
    "    x = identity_block(x, filters=256)\n",
    "    x = identity_block(x, filters=256)\n",
    "\n",
    "    # conv block 4\n",
    "    x = projection_block(x, filters=512, strides=(2, 2))\n",
    "    x = identity_block(x, filters=512)\n",
    "    x = identity_block(x, filters=512)\n",
    "\n",
    "    # global average pooling and dense layer\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # Skip connection\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out += residual  # Adding input back\n",
    "        return self.relu(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels, identity_downsample =None, stride=1):\n",
    "        super(block,self).__init__()\n",
    "        self.expansion =4 \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1,stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1,stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1,stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.Relu()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        x=self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x=self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x =self.relu(x)\n",
    "        return x\n",
    "       \n",
    "\n",
    "class ResNet(nn.Module):  #[3,4,6,3] how many time we want to reuse the block in a list\n",
    "    def __init__(self, block,layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels,64, kernel_size=7, stride=2,padding=3) #initial layer\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "\n",
    "        #ResNet layers\n",
    "        self.layer1 = self._make_layer(block,layers[0],out_channels=64, stride =1)\n",
    "        self.layer2 = self._make_layer(block,layers[1],out_channels=128, stride =2)\n",
    "        self.layer3 = self._make_layer(block,layers[2],out_channels=256, stride =2)\n",
    "        self.layer4 = self._make_layer(block,layers[3],out_channels=512, stride =2)  #2048 out_channels at the end\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc == nn.Linear(512*4, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x =self.bn1(x)\n",
    "        x = self.rule(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x=self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self,block, num_residual_block, out_channels, stride):\n",
    "        identity_downsample =None\n",
    "        layers = []\n",
    "\n",
    "        if stride !=1 or self.in_channels != out_channels * 4\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1, stride =stride),\n",
    "                                                nn.BatchNorm2d(out_channels*4))\n",
    "            \n",
    "        layers.append(block(self.in_channels, out_channels, identity_downsample,stride))\n",
    "        self.in_channels = self.out_channels*4 \n",
    "\n",
    "        for i in range(num_residual_block -1 ):\n",
    "            layers.append(block(self.in_channels, out_channels))  # input 256 -> 64 -> 256 again\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "\n",
    "def ResNet50(img_channels=3, num_classes=1000):\n",
    "    return ResNet(block, [3,4,6,3], img_channels, num_classes)\n",
    "\n",
    "def ResNet101(img_channels=3, num_classes=1000):\n",
    "    return ResNet(block, [3,4,23,3], img_channels, num_classes)  #change the numbr of layers\n",
    "\n",
    "def test():\n",
    "    net = ResNet50\n",
    "    x = torch.randn(2,3,224,224)\n",
    "    y = net(x).to('cuda')\n",
    "    print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
