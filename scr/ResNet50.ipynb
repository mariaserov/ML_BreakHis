{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b941b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import glob\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386bcb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/rds/general/user/ft824/home/ML_BreakHis/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c67e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#download pretrained weights \n",
    "#model = ResNet50(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a41946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pretrained weight without last layer\n",
    "resnet_weights_path = '/rds/general/user/ft824/home/.keras/models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "CHANNELS = 3\n",
    "IMAGE_RESIZE = 224\n",
    "NUM_CLASSES = 2 # change this to match your number of output classes\n",
    "DENSE_LAYER_ACTIVATION = 'sigmoid'  # use 'softmax' for categorical classification\n",
    "RESNET50_POOLING_AVERAGE = 'avg'  \n",
    "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
    "\n",
    "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
    "LOSS_METRICS = ['accuracy']\n",
    "\n",
    "# EARLY_STOP_PATIENCE must be < NUM_EPOCHS\n",
    "NUM_EPOCHS = 10\n",
    "EARLY_STOP_PATIENCE = 3\n",
    "\n",
    "# These steps value should be proper FACTOR of no.-of-images in train & valid folders respectively\n",
    "STEPS_PER_EPOCH_TRAINING = 10\n",
    "STEPS_PER_EPOCH_VALIDATION = 10\n",
    "\n",
    "#BATCH_SIZE sould be FACTOR of no of img in train and validation\n",
    "BATCH_SIZE_TRAINING = 32\n",
    "BATCH_SIZE_VALIDATION = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a710f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add pre-trained ResNet50 as the base (without the top classifier layer)\n",
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling=RESNET50_POOLING_AVERAGE,\n",
    "    weights=resnet_weights_path,\n",
    "    input_shape=(224, 224, 3)  # or your image size\n",
    "))\n",
    "\n",
    "# Freeze the base model, not to train first layer\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Add output layer for classification\n",
    "model.add(Dense(NUM_CLASSES, activation=DENSE_LAYER_ACTIVATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "650bd1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,098</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m4,098\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,591,810</span> (90.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,591,810\u001b[0m (90.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,098</span> (16.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,098\u001b[0m (16.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60bf03d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ft824/home/anaconda3/envs/breakhis/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define optimizer\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=sgd, loss=OBJECTIVE_FUNCTION, metrics=LOSS_METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "688dc8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5820 validated image filenames belonging to 2 classes.\n",
      "Found 1483 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load your CSV files\n",
    "train_df = pd.read_csv('old_metadata/augmented_train_dataset.csv')\n",
    "#augment_train = pd.read_csv('../data/augmented_dataset.csv')\n",
    "#train_df = pd.concat([train_df, augment_train], axis=0, ignore_index=True)\n",
    "\n",
    "test_df = pd.read_csv('old_metadata/new_test.csv')\n",
    "\n",
    "image_size = IMAGE_RESIZE  # for ResNet50\n",
    "\n",
    "\n",
    "# Define the ImageDataGenerator with preprocessing\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "# Training generator\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filepath',    # column with image file paths\n",
    "    y_col='label',       # column with image labels\n",
    "    target_size=(image_size, image_size),  # resizing to match ResNet50 input size\n",
    "    batch_size=BATCH_SIZE_TRAINING,\n",
    "    class_mode='categorical' # multi-class classification\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='filepath',    # column with image file paths\n",
    "    y_col='label',       # column with image labels\n",
    "    target_size=(image_size, image_size),  # resizing to match ResNet50 input size\n",
    "    batch_size=BATCH_SIZE_VALIDATION,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4c9310b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 182, 16, 93)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(BATCH_SIZE_TRAINING, len(train_generator), BATCH_SIZE_VALIDATION, len(test_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5438a360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ../train_aug/original_2621.png\n",
      "1    ../train_aug/original_4985.png\n",
      "2    ../train_aug/original_3990.png\n",
      "3    ../train_aug/original_2934.png\n",
      "4    ../train_aug/original_4068.png\n",
      "Name: filepath, dtype: object\n",
      "Missing files: 0\n",
      "Empty DataFrame\n",
      "Columns: [filepath, label, magnification, tumor_subtype]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "##check for missing files\n",
    "print(train_df['filepath'].head())\n",
    "missing = train_df[~train_df['filepath'].apply(os.path.exists)]\n",
    "print(f\"Missing files: {len(missing)}\")\n",
    "print(missing.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4b17c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping & checkpointing the best model in ../working dir & restoring that as our model for prediction\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE)\n",
    "cb_checkpointer = ModelCheckpoint(filepath = '../working/best.weights.h5', monitor = 'val_loss', save_best_only = True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7ecd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 27s/step - accuracy: 0.8028 - loss: 0.7943 - val_accuracy: 0.7937 - val_loss: 0.7476\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 34s/step - accuracy: 0.8280 - loss: 0.5253 - val_accuracy: 0.8750 - val_loss: 0.4148\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 35s/step - accuracy: 0.8854 - loss: 0.4089 - val_accuracy: 0.7812 - val_loss: 0.7345\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 35s/step - accuracy: 0.8247 - loss: 0.6729 - val_accuracy: 0.8438 - val_loss: 0.6105\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 33s/step - accuracy: 0.9043 - loss: 0.3639 - val_accuracy: 0.8562 - val_loss: 0.6322\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_TRAINING,\n",
    "        epochs = NUM_EPOCHS,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=STEPS_PER_EPOCH_VALIDATION,\n",
    "        callbacks=[cb_checkpointer, cb_early_stopper]\n",
    ")\n",
    "model.load_weights(\"../working/best.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe633df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6afee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy function\n",
    "def check_accuracy(output,labels):\n",
    "    _,predpos=output.max(1)\n",
    "    num_samples=len(labels)\n",
    "    num_correct=(predpos==labels).sum()\n",
    "    return (num_correct/num_samples)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state,filename='clahe.pth.tar'):\n",
    "    print('Saving weights-->')\n",
    "    torch.save(state,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename):\n",
    "    print('Loading weights-->')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optim.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "num_workers=2\n",
    "learning_rate=0.001\n",
    "print(device)\n",
    "num_epochs=25\n",
    "load_model=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resnet model, with respecitve transform\n",
    "model = models.resnet50(pretrained=False)\n",
    "model.fc=nn.Sequential(nn.Linear(2048,1024),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Linear(1024,512),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Linear(512,2))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optim=torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load('weights.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add04b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load data\n",
    "#train, validation, test\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,num_workers=num_workers, shuffle=True)\n",
    "validation_loader = DataLoader(valid_set, batch_size=batch_size,num_workers=num_workers,shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size,num_workers=num_workers,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f305bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Put model on cuda \n",
    "model.to(device)\n",
    "# Put the model on train mode\n",
    "model.train()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14216ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,y=next(iter(train_loader))\n",
    "i=i.to(device)\n",
    "y=y.to(device)\n",
    "y_pred=model(i)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for the model\n",
    "min_loss=None\n",
    "for epoch in range(num_epochs):\n",
    "    losses=[]\n",
    "    accuracies=[]\n",
    "    loop= tqdm(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "    for batch_idx, (data,labels) in loop:\n",
    "        # Put data on cuda\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device).long()\n",
    "        \n",
    "        # Forward pass\n",
    "        output=model(data)\n",
    "        \n",
    "        # Find out loss\n",
    "        loss=criterion(output,labels)\n",
    "        accuracy=check_accuracy(output,labels)\n",
    "        losses.append(loss.detach().item())\n",
    "        accuracies.append(accuracy.detach().item())\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step\n",
    "        optim.step()\n",
    "        \n",
    "        # Update TQDM progress bar\n",
    "        loop.set_description(f\"Epoch [{epoch}/{num_epochs}] \")\n",
    "        loop.set_postfix(loss=loss.detach().item(),accuracy=accuracy.detach().item())\n",
    "        \n",
    "    moving_loss=sum(losses)/len(losses)\n",
    "    moving_accuracy=sum(accuracies)/len(accuracies)\n",
    "    checkpoint={'state_dict': model.state_dict(),'optimizer': optim.state_dict()}\n",
    "    # Save check point\n",
    "    if min_loss==None:\n",
    "        min_loss=moving_loss\n",
    "        save_checkpoint(checkpoint)\n",
    "    elif moving_loss<min_loss:\n",
    "        min_loss=moving_loss\n",
    "        save_checkpoint(checkpoint)\n",
    "    print('Epoch {0} : Loss = {1} , Accuracy={2}'.format(epoch,moving_loss,moving_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af13acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation accuracy\n",
    "correct=0\n",
    "samples=0\n",
    "for data,labels in validation_loader:\n",
    "    data=data.to(device)\n",
    "    labels=labels.to(device)\n",
    "    # Forward pass\n",
    "    y_pred=model(data)\n",
    "    # Accuracy over entire dataset\n",
    "    _,predpos=y_pred.max(1)\n",
    "    samples+=len(labels)\n",
    "    correct+=(predpos==labels).sum().detach().item()\n",
    "print('Validation accuracy : ',(correct/samples)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test accuracy\n",
    "correct=0\n",
    "samples=0\n",
    "for data,labels in test_loader:\n",
    "    data=data.to(device)\n",
    "    labels=labels.to(device)\n",
    "    # Forward pass\n",
    "    y_pred=model(data)\n",
    "    # Accuracy over entire dataset\n",
    "    _,predpos=y_pred.max(1)\n",
    "    samples+=len(labels)\n",
    "    correct+=(predpos==labels).sum().detach().item()\n",
    "print('Test accuracy : ',(correct/samples)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breakhis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
